[
["index.html", "BayesLab - Exploring Bayesian Hierarchical Modeling Preface", " BayesLab - Exploring Bayesian Hierarchical Modeling Dean Billheimer 2017-09-24 Preface To Dean: Bayesian inference in models with linear structure (e.g. linear models) offers advantages in estimation and inference by “borrowing strength” across similar groups. I don’t really understand exactly how this works. This ebook is explores theoretical and practical advantages of this approach. Specifically, there are multiple issues I want to understand: Efficiency gains in factorial experiments. By way of example, suppose I have a two-factor experiment (A and B) each with two levels. The full factorial structure consists of ab, aB, Ab, and AB. Further, suppose A is the primary factor of interest, and B is more exploratory (think predictive biomarker). A standard sample size calculation (\\(\\alpha = 0.05\\), power 0.8) evaluating A only requires \\(n=17\\) EUs per group, for each of two groups (a and A). Conversely, the full factorial suggests \\(n=12\\) for each of the four groups to test the main A effect (via linear contrast). What’s up with that? It “feels” like the first analysis (two group) puts all posterior probability on the “no B effect” hypothesis, while the second assumes the general hypothesis of four disparate groups. Neither of these is satisfactory. What do we do with sample size? Maybe information gain or other metrics are better criteria that power? For phase III trials with FDA review, fixed significance level \\(\\alpha = 0.05\\), and high power (\\(&gt; 80\\)%) may be (maybe?) appropriate. What about for laboratory studies with mice? or translational studies with prospectively sampled tissue specimens? or a phase I/II study with a single dose? or a single arm trial against a historical control? close by, when is response adaptive randomization worthwhile? How does explicit hierarchical modeling improve upon sequential modeling based on sufficient statistics? Superficially, any advantages from hierarchical modeling would seem to accrue from Stein’s result regarding MSE (EMSE?) for estimation of multiple parameters. Is this the only phenomenon at work here? I think the information encoded in prior distributions on parameters is subtle. "],
["introduction-a-first-example-.html", "Chapter 1 Introduction - A first example. 1.1 A Scientist’s Analysis 1.2 More Standard Classical Analysis 1.3 Bayesian Hierarchical Modeling 1.4 Fundamental Problem", " Chapter 1 Introduction - A first example. Suppose we have a simple experiment with two factors (A/B), each with two levels (aA, bB), conducted in a factorial design with \\(n=10\\) observations per group. Our general hypothesis is that either factor A or factor B might increase the response, or that both together might be required to create an increase. Let’s consider some data (well, frauda = ‘fraudulent data’), for these four groups (shown below). 1.1 A Scientist’s Analysis A typical “laboratory” analysis would consider group comparisons using \\(t\\)-tests. For ab vs. Ab Welch Two Sample t-test data: y[group == &quot;ab&quot;] and y[group == &quot;Ab&quot;] t = -1.4267, df = 16.739, p-value = 0.1721 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -1.6205353 0.3139597 sample estimates: mean of x mean of y 3.026748 3.680035 For ab vs. aB Welch Two Sample t-test data: y[group == &quot;ab&quot;] and y[group == &quot;aB&quot;] t = -1.443, df = 17.057, p-value = 0.1671 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -1.6523020 0.3098862 sample estimates: mean of x mean of y 3.026748 3.697956 and finally for ab vs. AB Welch Two Sample t-test data: y[group == &quot;ab&quot;] and y[group == &quot;AB&quot;] t = -3.4484, df = 17.79, p-value = 0.002907 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -3.0392917 -0.7367985 sample estimates: mean of x mean of y 3.026748 4.914793 A simple interpretation is that neither A (\\(p =\\) 0.17) nor B (\\(p =\\) 0.17) alone produce an increase, but A and B together ‘interact’ to create an increase in \\(y\\). Clearly, there are issues here. But note that even if I use a Bonferroni correction for multiple testing, the interaction p-value remains significant (\\(p =\\) 0.009). I think many lab scientists would not see a huge problem here, and I’m pretty sure this could be published in a good journal. 1.2 More Standard Classical Analysis A slightly different (better?) analysis considers the groups (and factors) together. This is the standard two-way factorial analysis. “Better” here, depends on your hypotheses. Results are shown in the tables below. Table 1.1: ANOVA for two-factor experiment Df Sum Sq Mean Sq F value Pr(&gt;F) a.fac 1 8.743 8.743 7.627 0.009 b.fac 1 9.082 9.082 7.922 0.008 a.fac:b.fac 1 0.794 0.794 0.693 0.411 Residuals 36 41.271 1.146 NA NA Call: lm(formula = y ~ a.fac * b.fac) Residuals: Min 1Q Median 3Q Max -2.00535 -0.73052 0.09802 0.71468 1.81550 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 3.0267 0.3386 8.939 1.14e-10 *** a.facA 0.6533 0.4788 1.364 0.181 b.facB 0.6712 0.4788 1.402 0.170 a.facA:b.facB 0.5635 0.6772 0.832 0.411 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.071 on 36 degrees of freedom Multiple R-squared: 0.3109, Adjusted R-squared: 0.2535 F-statistic: 5.414 on 3 and 36 DF, p-value: 0.003531 The “textbook” interpretation (from a second stats class) is there are significant A and B effects, but no A:B interaction (again, based on p-values). This is the exact opposite of the simple-approach analysis above. WTF! It’s not really fair to blame p-values for this, but their use doesn’t help the situation. More carefully, the interpretaion can be complicated. If there is truly no interaction, then the A and B main effects are estimated efficiently using all observations, and we can conclude ‘significant A and B main effects’. The SS partitioning does not indicate a strong effect of the interaction. But zero? We know interaction tests often suffer from - low power - awkward interpretation - potential mismatch between scientific and statisitcal hypotheses If you can’t rule out the interaction (e.g., via an equivalence test or similar), do you want to assume it is zero? If an interaction is present, then clean estimation of the main effects is difficult. If the interaction is not assumed zero, Classical procedure reverts to pairwise comparisons of groups (and the simple scientist’s analysis is not far off). Conversely, assuming no interaction is equivalent to accepting the interaction null hypothesis (a classical, knee-jerk no-no). Also, because the A:B interaction is part of the original hypothesis to be evaluated, this assumption seems problematic, or at least philosophically suspect. Many of the issues here appear to result from a forced dichotomization or conditioning on intermediate results. Is the A:B interaction present or not? Conditional on this answer, we choose a next analysis procedure. Surely the conditional analysis must have an effect on Type I error rate. (It obviously effects Type II errors.) 1.3 Bayesian Hierarchical Modeling In the Bayesian approach, we compute posterior probabities of unknown quantities (here, means and variances). SAMPLING FOR MODEL &#39;gaussian(identity) brms-model&#39; NOW (CHAIN 1). Chain 1, Iteration: 1 / 2000 [ 0%] (Warmup) Chain 1, Iteration: 200 / 2000 [ 10%] (Warmup) Chain 1, Iteration: 400 / 2000 [ 20%] (Warmup) Chain 1, Iteration: 600 / 2000 [ 30%] (Warmup) Chain 1, Iteration: 800 / 2000 [ 40%] (Warmup) Chain 1, Iteration: 1000 / 2000 [ 50%] (Warmup) Chain 1, Iteration: 1001 / 2000 [ 50%] (Sampling) Chain 1, Iteration: 1200 / 2000 [ 60%] (Sampling) Chain 1, Iteration: 1400 / 2000 [ 70%] (Sampling) Chain 1, Iteration: 1600 / 2000 [ 80%] (Sampling) Chain 1, Iteration: 1800 / 2000 [ 90%] (Sampling) Chain 1, Iteration: 2000 / 2000 [100%] (Sampling) Elapsed Time: 0.142641 seconds (Warm-up) 0.16664 seconds (Sampling) 0.309281 seconds (Total) SAMPLING FOR MODEL &#39;gaussian(identity) brms-model&#39; NOW (CHAIN 2). Chain 2, Iteration: 1 / 2000 [ 0%] (Warmup) Chain 2, Iteration: 200 / 2000 [ 10%] (Warmup) Chain 2, Iteration: 400 / 2000 [ 20%] (Warmup) Chain 2, Iteration: 600 / 2000 [ 30%] (Warmup) Chain 2, Iteration: 800 / 2000 [ 40%] (Warmup) Chain 2, Iteration: 1000 / 2000 [ 50%] (Warmup) Chain 2, Iteration: 1001 / 2000 [ 50%] (Sampling) Chain 2, Iteration: 1200 / 2000 [ 60%] (Sampling) Chain 2, Iteration: 1400 / 2000 [ 70%] (Sampling) Chain 2, Iteration: 1600 / 2000 [ 80%] (Sampling) Chain 2, Iteration: 1800 / 2000 [ 90%] (Sampling) Chain 2, Iteration: 2000 / 2000 [100%] (Sampling) Elapsed Time: 0.182641 seconds (Warm-up) 0.133657 seconds (Sampling) 0.316298 seconds (Total) SAMPLING FOR MODEL &#39;gaussian(identity) brms-model&#39; NOW (CHAIN 3). Chain 3, Iteration: 1 / 2000 [ 0%] (Warmup) Chain 3, Iteration: 200 / 2000 [ 10%] (Warmup) Chain 3, Iteration: 400 / 2000 [ 20%] (Warmup) Chain 3, Iteration: 600 / 2000 [ 30%] (Warmup) Chain 3, Iteration: 800 / 2000 [ 40%] (Warmup) Chain 3, Iteration: 1000 / 2000 [ 50%] (Warmup) Chain 3, Iteration: 1001 / 2000 [ 50%] (Sampling) Chain 3, Iteration: 1200 / 2000 [ 60%] (Sampling) Chain 3, Iteration: 1400 / 2000 [ 70%] (Sampling) Chain 3, Iteration: 1600 / 2000 [ 80%] (Sampling) Chain 3, Iteration: 1800 / 2000 [ 90%] (Sampling) Chain 3, Iteration: 2000 / 2000 [100%] (Sampling) Elapsed Time: 0.158778 seconds (Warm-up) 0.218157 seconds (Sampling) 0.376935 seconds (Total) SAMPLING FOR MODEL &#39;gaussian(identity) brms-model&#39; NOW (CHAIN 4). Chain 4, Iteration: 1 / 2000 [ 0%] (Warmup) Chain 4, Iteration: 200 / 2000 [ 10%] (Warmup) Chain 4, Iteration: 400 / 2000 [ 20%] (Warmup) Chain 4, Iteration: 600 / 2000 [ 30%] (Warmup) Chain 4, Iteration: 800 / 2000 [ 40%] (Warmup) Chain 4, Iteration: 1000 / 2000 [ 50%] (Warmup) Chain 4, Iteration: 1001 / 2000 [ 50%] (Sampling) Chain 4, Iteration: 1200 / 2000 [ 60%] (Sampling) Chain 4, Iteration: 1400 / 2000 [ 70%] (Sampling) Chain 4, Iteration: 1600 / 2000 [ 80%] (Sampling) Chain 4, Iteration: 1800 / 2000 [ 90%] (Sampling) Chain 4, Iteration: 2000 / 2000 [100%] (Sampling) Elapsed Time: 0.150101 seconds (Warm-up) 0.140182 seconds (Sampling) 0.290283 seconds (Total) Family: gaussian (identity) Formula: y ~ 1 + (1 | group) Data: df01 (Number of observations: 40) Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; total post-warmup samples = 4000 WAIC: Not computed Group-Level Effects: ~group (Number of levels: 4) Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat sd(Intercept) 1.33 0.98 0.31 4.11 517 1.02 Population-Level Effects: Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat Intercept 3.89 0.8 2.4 5.71 409 1.01 Family Specific Parameters: Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat sigma 1.11 0.14 0.89 1.41 1903 1 Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1). Table shows mean and quantile estimates of the group means. kable(mcmc.parEst, booktabs=TRUE, digits=2, caption=&quot;Group mean estimates and quantiles from Bayesian model&quot;) Table 1.2: Group mean estimates and quantiles from Bayesian model mean 2.5% 10% 50% 90% 97.5% ab 3.15 2.45 2.70 3.15 3.60 3.83 Ab 3.71 3.06 3.29 3.70 4.12 4.37 aB 3.72 3.08 3.30 3.71 4.13 4.36 AB 4.76 4.00 4.30 4.76 5.22 5.45 Figure (fig:newfig} shows that Bayesian estimates ‘borrow’ strength’ across groups and are shrunk toward the overall mean of all groups. In this example, the Bayesian credible intervals are approximately the same length as the classical confidence intervals (about 0.990.990.990.99%). I also show Bayesian 80% posterior credible intervals. In general, these intervals are about 2/3 as long as 95% intervals, but still retain substantial posterior probability for the mean value (4 chances in 5, pretty good, que no?). IMO this narrower interval helps us focus on the quantities we care about. 1.3.1 Estimating the A effect To estimate the effect of treatment A, let’s check its effect in the absense (b) and presence (B) of treatment B. Figure shows estimated densities (distributions) for the mean difference associated with A when B is absent (black curve), and when B is present (red). Table 1.3: Mean effect of A treatment with and without B mean 2.5% 10% 50% 90% 97.5% A - a w/ b 0.56 -0.38 -0.07 0.56 1.17 1.49 A - a w/ B 1.04 0.06 0.43 1.04 1.66 1.99 The figure shows that the curves are centered on mean estimates of 0.5, and 1, respectively, indicating (perhaps) a positive effect of A. When B is absent, the probability of no effect (or negative effect) is 0.13 and when B is present is 0.02. Combining curves (marginalizing over B) results in probability 0.07. To me these results suggest some (weakish) evidence of an A effect regardless of the level of B, and somwhat stronger evidence of an A effect in the presence of B. What, specifically, was the original hypothesis about A and B? This clearly (now) has a bearing on our focus and interpretation. 1.3.2 A:B Interaction The conditional analysis in the previous subsection doesn’t directly address the A:B interaction. We’ll do that here. Figure shows a density plot for the estimated interaction effect. The mean estimate is about 0.5, and the 80% interval ranges from about -0.4 to 1.4. Greater uncertainty associated with this effect reduces the evidence of a positive effect. The probability that the effect is zero (or negative) is 0.24. The Bayesian analysis provides a similar interpretation of results as the factorial classical analysis. But, I think the focus on conditional pairwise comparisons makes it more palatable to non-statistical scientists. Here, the hierarchical modeling is used to address multiple comparisons issues (a la’ Gelman), but is not fundamental to the solution. It’s real benefit is to help us focus on effect sizes and interpretation of scientific hypotheses. 1.4 Fundamental Problem I think the fundamental problem is with dichotomization of results and conditional analysis procedures; not conditional probabilities, but procedure choices that are conditional on earlier stage results. BTW - what does interaction mean in this problem? "],
["diving-in.html", "Chapter 2 Diving In", " Chapter 2 Diving In Now let’s talk details. summary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 plot(cars) "]
]
