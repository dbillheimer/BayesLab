# Introduction - A first example.

Suppose we have a simple experiment with two factors (A/B), each with two levels
(aA, bB), conducted in a factorial design with $n=10$ observations per group.
Our general hypothesis is that either factor A or factor B might increase the
response, or that both together might be required to create an increase.  Let's
consider some data (well, frauda = 'fraudulent data'), for these four groups
(shown below).


```{r init, echo=F}
library(knitr)
Hmisc::knitrSet(w=7, h=5, lang='markdown') ## omit the ## at the beginning of R output
## lines
##args(Hmisc::knitrSet)
```

```{r datasim, fig.width=6, echo=F}
library(ggplot2)
n <- 10
group <- gl(4, 10, labels=c("ab", "Ab", "aB", "AB"))
mu <- kronecker(c(3, 4, 4, 5), rep(1, 10))
set.seed(1959)
y <- rnorm(4 * n, mu, 1)
df01 <- data.frame(group=group, y=y)
qplot(group, y, geom=c("boxplot", "point") ) +
    ggtitle("Observations from a two-factor experiment")

```

## A Scientist's Analysis

A typical "laboratory" analysis would consider group comparisons using
$t$-tests.

For ab vs. Ab
```{r ab.v.Ab, echo=F}
tt1 <- t.test(y[group=="ab"], y[group=='Ab'])
print(tt1)
```
For ab vs. aB
```{r ab.v.aB, echo=F}
tt2 <- t.test(y[group=="ab"], y[group=='aB'])
print(tt2)
```

and finally for ab vs. AB
```{r ab.v.AB, echo=F}
tt3 <- t.test(y[group=="ab"], y[group=='AB'])
print(tt3)
```

A simple interpretation is that neither A ($p =$ `r round(tt1$p.value,2)`) nor B
($p =$ `r round(tt2$p.value, 2)`) alone produce an increase, but A and B together
'interact' to create an increase in $y$.


```{r stars, echo=F}
qplot(group, y, geom=c("boxplot", "point") ) +
    ggtitle("Observations from a two-factor experiment") +
    geom_segment(aes(x=c(1,1,1), xend=c(2,3,4), y=c(6, 6.4, 6.8), yend=c(6, 6.4, 6.8)),
          arrow=arrow(length=unit(0.1, "cm"), angle=90, ends="both") ) +
    geom_text(aes(x=c(1.5, 2.5, 3.5), y=c(6.15, 6.55, 6.95)),
              label=c(paste("p=", round(tt1$p.value, 2)),
                  paste("p=", round(tt2$p.value, 2)),
                  paste("p=", round(tt3$p.value, 3)) ), cex=3)
```

Clearly, there are issues here. But note that even if I use a Bonferroni
correction for multiple testing, the interaction p-value remains significant
($p =$ `r round(tt3$p.value *3, 3)`).  I think many lab scientists would
not see a huge problem here, and I'm pretty sure this could be published in a
good journal.


## More Standard Classical Analysis
A slightly different (better?) analysis considers the groups (and factors)
together. This is the standard two-way factorial analysis.
"Better" here, depends on your hypotheses.  Results are shown in the tables below.

```{r anova, echo=F}
a.fac <- factor(rep(c(rep(0, 10), rep(1, 10)), 2), labels=c("a", "A"))
b.fac <- factor(c(rep(0, 20), rep(1, 20)), labels=c("b", "B"))
fac.lm <- lm(y ~ a.fac*b.fac)
kable(anova(fac.lm), digits=3, caption="ANOVA for two-factor experiment")
summary(fac.lm)
```

The "textbook" interpretation (from a second stats class) is there are
significant A and B effects, but no 
A:B interaction (again, based on p-values).  This is the exact opposite of the
simple-approach analysis above.  WTF!  It's not really fair to blame p-values for
this, but their use doesn't help the situation.


More carefully, the interpretaion can be complicated.
If there is truly no interaction, then the A and B main effects are
estimated efficiently using all observations, and we can conclude 'significant A
and B main effects'. The SS partitioning does not indicate a strong effect of the interaction.  But
zero? We know interaction tests often suffer from
- low power
- awkward interpretation
- potential mismatch between scientific and statisitcal hypotheses

If you can't rule out the interaction (e.g., via an equivalence test or
similar), do you want to assume it is zero?

If an interaction is present, then clean estimation of the main effects is
difficult. If the interaction is not assumed zero, Classical procedure reverts
to pairwise comparisons of groups (and the simple scientist's analysis is not
far off).

Conversely, assuming no interaction is equivalent to
accepting the interaction null hypothesis (a classical, knee-jerk no-no).
Also, because the A:B interaction is part of the original hypothesis to be
evaluated, this assumption seems problematic, or at least philosophically suspect.


```{r gmods, echo=F}
library(gmodels)
model.mat <- rbind("ab"=c(1, 0, 0, 0),
                   "Ab"=c(1, 1, 0, 0),
                   "aB"=c(1, 0, 1, 0),
                   "AB"=c(1, 1, 1 ,1))
fac.lsm <- estimable(fac.lm, model.mat)
## estimable(fac.lm, rbind("Ab - ab"= c(model.mat[2,] - model.mat[1,])))
## estimable(fac.lm, rbind("A Effect"= c(model.mat[4,] + model.mat[2,] -
##                                          (model.mat[3,] + model.mat[1,]))*.5))
## estimable(fac.lm, rbind("B Effect"= c(model.mat[4,] + model.mat[3,] -
##                                          (model.mat[2,]+ model.mat[1,]))*.5))
## estimable(fac.lm, rbind("AB Inter"= c(model.mat[4,] - model.mat[3,] -
##                                           model.mat[2,]+ model.mat[1,])*.5))

lsm.df <- data.frame(lsmean=fac.lsm$Estimate,
                     lower.CL = fac.lsm$Estimate + qt(.025, fac.lsm$DF) *
                         fac.lsm$Std.,
                     upper.CL = fac.lsm$Estimate + qt(.9755, fac.lsm$DF) *
                         fac.lsm$Std.) 
##lsm.df                         
```

Many of the issues here appear to result from a forced dichotomization or
conditioning on intermediate results. Is the A:B interaction present or not?
Conditional on this answer, we choose a next analysis procedure. Surely the
conditional analysis must have an effect on Type I error rate. (It obviously
effects Type II errors.)

## Bayesian Hierarchical Modeling
In the Bayesian approach, we compute posterior probabities of unknown quantities
(here, means and variances).

```{r hibayes, cache=T, echo=F, results="hide"}
library(xtable)
library(brms)
fit1 <- brm(formula=y ~ 1 + (1 | group), data=df01,
            warmup=1000, iter=2000, control=list(adapt_delta=0.9))

summary(fit1)

post1 <- posterior_samples(fit1)
## str(post1)  ## may be a useful approach
## grep("Ab", names(post1))
## grep("ab", names(post1))

m1.ab <- apply( post1[c(1, 4)], 1, sum)
m1.Ab <- apply( post1[c(1, 5)], 1, sum)
m1.aB <- apply( post1[c(1, 6)], 1, sum)
m1.AB <- apply( post1[c(1, 7)], 1, sum)

mc.sum <- function(x) c(mean=mean(x), quantile(x, c(0.025, 0.1, 0.5, 0.9, 0.975)))

mcmc.parEst <- rbind("ab" = mc.sum(m1.ab), "Ab" = mc.sum(m1.Ab),
                     "aB" = mc.sum(m1.aB), "AB" = mc.sum(m1.AB))
parEst.group <- as.data.frame(mcmc.parEst)
names(parEst.group) <- c("mean", "q.02", "q.10", "q.50", "q.90", "q.98")
## parEst.group
```
Table \ref{tab:outtable} shows mean and quantile estimates of the group means.

```{r outtable}
kable(mcmc.parEst, booktabs=TRUE, digits=2,
      caption="Group mean estimates and quantiles from Bayesian model")
```

```{r groupfig, echo=F}
graph.offset <- 0.45
g.offset2 <- 0.55
## function to graph data, Bayes, and LSM results
group.compare <- function(x=group, y=fev1, data.df=tmp.df, bayes.df, lsm.df, title="") {
    ggplot(aes(x=group, y=y), data=data.df) +
        geom_boxplot(outlier.size=0, outlier.alpha=0) +
            geom_jitter(width=.2) +
    geom_segment(aes(x=c(1:4)+graph.offset, y=q.02, xend=c(1:4)+graph.offset, yend=q.98),
                 data=bayes.df, col=2) +
                     geom_segment(aes(x=c(1:4)+ graph.offset,
                                      y=q.10, xend=c(1:4)+graph.offset, yend=q.90),
                                  data=bayes.df, col=2, lwd=3) +
    geom_point(aes(x=c(1:4)+graph.offset, y=mean), data=bayes.df, pch=3, col=1) + 
        geom_segment(aes(x=c(1:4)+g.offset2, y=lower.CL, xend=c(1:4)+g.offset2, yend=upper.CL),
                     data=lsm.df, col='blue') +
                         geom_point(aes(x=c(1:4)+g.offset2, y=lsmean),
                                    data=lsm.df, col='blue') +
        ggtitle(title)
}

```

```{r setfigcap1, echo=F}
newfig.cap <-
    c("Observations for group A:B experiments with classical (blue) and Bayesian (red) estimates of
group means.  Blue indicates point estimate (dots) and 95% confidence intervals (lines). 
+ indicates Bayesian mean point estimate with 80% (thick red) and 95% (thin red) intervals.")
```

```{r newfig, fig.width=7, echo=F, fig.caption=newfig.cap}
set.seed(1959)
group.compare(x=group, y=y, df01, bayes.df=parEst.group , lsm.df=lsm.df,
              title="Data and mean estimates for A and B treatment groups")

```

```{r calc, echo=F}
int.ratio <- mean(parEst.group[,6] - parEst.group[,2])/(lsm.df[,3] - lsm.df[,2])
##mean((parEst.group[,5] - parEst.group[,3])/(parEst.group[,6] - parEst.group[,2]))
```

Figure \ref(fig:newfig} shows that Bayesian estimates 'borrow' strength'
across groups and are shrunk toward the overall mean of all groups. In this
example, the Bayesian credible intervals are approximately the same length as
the classical confidence intervals (about `r round(int.ratio, 2)`%).  I also
show Bayesian 80% posterior credible intervals. In general, these intervals are
about 2/3 as long as 95% intervals, but still retain substantial posterior
probability for the mean value (4 chances in 5, pretty good, que no?).  IMO this
narrower interval helps us focus on the quantities we care about.

### Estimating the A effect
To estimate the effect of treatment A, let's check its effect in the absense (b)
and presence (B) of treatment B. Figure \ref{fig:A-effect} shows estimated
densities (distributions) for the mean difference associated with A when B is
absent (black curve), and when B is present (red).

```{r A-effect, echo=F, fig.caption="Estimated A effect with and without factor B", fig.width=7}
set.seed(1959)
dA.b <- m1.Ab - sample(m1.ab) ## A - a | b
dA.B <- m1.AB - sample(m1.aB) ## A - a | B
qplot(x=dA.b , geom="density", xlab="Estimated A effect mean") +
    geom_density(aes(x=dA.B), col=2) +
    geom_text(aes(x=c(-0.5, 2.0), y=c(0.7, 0.7)),
              label=c("A-a effect | b", "A-a effect | B"), col=c(1,2), cex=5) +
                  ggtitle("A - a Effect Conditional on B Treatment")

kable(rbind("A - a w/ b"= mc.sum(dA.b), "A - a w/ B" = mc.sum(dA.B)),
      booktabs=T, digits=2,
      caption="Mean effect of A treatment with and without B")
```

```{r A-probs, echo=F}
dA.. <- c(dA.b, dA.B)
pA.b <- sum(dA.b < 0)/length(dA.b) ## 0.129
pA.B <- sum(dA.B < 0)/length(dA.B) ## 0.018
pA.. <-sum(dA.. < 0)/length(dA..) ## 0.073

```

The figure shows that the curves are centered on mean estimates of 0.5, and 1, respectively,
indicating (perhaps) a positive effect of A.  When B is absent, the probability
of no effect (or negative effect) is `r round(pA.b, 2)` and when B is present is
`r round(pA.B, 2)`. Combining curves (marginalizing over B) results in
probability `r round(pA.., 2)`. To me these results suggest some (weakish)
evidence of an A effect regardless of the level of B, and somwhat stronger
evidence of an A effect in the presence of B.

What, specifically, was the original hypothesis about A and B?  This clearly
(now) has a bearing on our focus and interpretation.

### A:B Interaction
The conditional analysis in the previous subsection doesn't directly address the
A:B interaction. We'll do that here.

```{r ABinter, echo=F, fig.width=7, fig.caption="Estimated interaction of A and B treatments"}
i.AB <- dA.B - sample(dA.b)
p.AB <- sum(i.AB < 0)/length(i.AB) ## .24
mcsum.AB <- mc.sum(i.AB)

qplot(x=i.AB , geom="density", xlab="Estimated A:B interaction effect") +
    geom_segment(aes(x=mcsum.AB[c(2)], y=c(.02), xend=mcsum.AB[c(6)],
                     yend=c(0.02)), col=c(2), lwd=c(1)) +
    geom_segment(aes(x=mcsum.AB[c(3)], y=c(0.02), xend=mcsum.AB[c(5)],
                     yend=c(0.02)), col=c(2), lwd=c(3)) +
    geom_point(aes(x=mcsum.AB[1], y=0.02), pch=3, col=1, cex=4) +
            ggtitle("Estimated A:B interaction effect")
```

Figure \ref{fig:ABinter} shows a density plot for the estimated interaction
effect.  The mean estimate is about 0.5, and the 80% interval ranges from about
-0.4 to 1.4. Greater uncertainty associated with this effect reduces the
evidence of a positive effect.  The probability that the effect is zero (or
negative) is `r round(p.AB, 2)`.

The Bayesian analysis provides a similar interpretation of results as the
factorial classical analysis.  But, I think the focus on conditional pairwise
comparisons makes it more palatable to non-statistical scientists.  Here, the
hierarchical modeling is used to address multiple comparisons issues (a la'
Gelman), but is not fundamental to the solution. It's real benefit is to help us
focus on effect sizes and interpretation of scientific hypotheses.

## Fundamental Problem
I think the fundamental problem is with dichotomization of results and
conditioning subsequent analyses on this choice. These are not conditional
probabilities, but procedure choices that are conditional on earlier stage
results. 

BTW - what does interaction mean in this problem?
